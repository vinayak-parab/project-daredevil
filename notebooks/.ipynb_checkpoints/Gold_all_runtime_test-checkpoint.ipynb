{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "from src.datasets.soccernet_generic_wcombined import soccernet_dataset_generic\n",
    "from src.utils.helper import samples_by_language\n",
    "import src.utils.training_helper as training_helper\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/work/oarongve/data/sound_dataset/SoccerNet-code/data/\"\n",
    "train_list = \"/work/oarongve/data/sound_dataset/SoccerNet-code/data/listgame_Train_300.npy\"\n",
    "valid_list = \"/work/oarongve/data/sound_dataset/SoccerNet-code/data/listgame_Valid_100.npy\"\n",
    "test_list = \"/work/oarongve/data/sound_dataset/SoccerNet-code/data/listgame_Test_100.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 16 20:00:38 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:57:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    64W / 350W |   1306MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     58777      C   .../project-daredevil-8eBKzQn6/bin/python3  1293MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{tabular}{lr}\n",
    "\\toprule\n",
    "{} &         0 \\\\\n",
    "\\midrule\n",
    "Avg. Time per match. Log-Mel-Spectrogram generation(seconds)    &  5.27 \\\\\n",
    "Avg. Visual model forward pass      &  0.144913 \\\\\n",
    "Avg. Audio model forward pass &\n",
    "Avg. Softmax average forward pass &\n",
    "Avg. Concatenation forward pass &\n",
    "\n",
    "# Audio\n",
    "2 7.309828049097306e-05\n",
    "4 0.001966991302294609\n",
    "8 9.376684824625652e-05\n",
    "16 0.0019805265084291116\n",
    "32 0.00012131690979003906\n",
    "\n",
    "# Combined\n",
    "2 6.600917913974859e-05\n",
    "4 0.00045068056155473757\n",
    "8 7.192049271021134e-05\n",
    "16 0.00045410046210655797\n",
    "32 8.707376626821664e-05\n",
    "\n",
    "# Visual\n",
    "2 6.353097084241035e-05\n",
    "4 0.00045702543014135115\n",
    "8 6.658810835618239e-05\n",
    "16 0.00045607310075026293\n",
    "32 7.807438190166766e-05\n",
    "\n",
    "# Softmax\n",
    "\n",
    "2 9.959245339418068e-05\n",
    "4 0.003238519521859976\n",
    "8 0.0001233503146049304\n",
    "16 0.0032860434360993214\n",
    "32 0.0001542040017934946\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio\n",
    "2 7.309828049097306e-05\n",
    "4 0.001966991302294609\n",
    "8 9.376684824625652e-05\n",
    "16 0.0019805265084291116\n",
    "32 0.00012131690979003906\n",
    "\n",
    "# Combined\n",
    "2 6.600917913974859e-05\n",
    "4 0.00045068056155473757\n",
    "8 7.192049271021134e-05\n",
    "16 0.00045410046210655797\n",
    "32 8.707376626821664e-05\n",
    "\n",
    "# Visual\n",
    "2 6.353097084241035e-05\n",
    "4 0.00045702543014135115\n",
    "8 6.658810835618239e-05\n",
    "16 0.00045607310075026293\n",
    "32 7.807438190166766e-05\n",
    "\n",
    "# Softmax\n",
    "\n",
    "2 9.959245339418068e-05\n",
    "4 0.003238519521859976\n",
    "8 0.0001233503146049304\n",
    "16 0.0032860434360993214\n",
    "32 0.0001542040017934946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "langpath_valid = '/work/oarongve/project-daredevil/project-daredevil/language-annotations/annotations/valid_lang_dict.json'\n",
    "samples_valid_all = samples_by_language(langpath_valid,valid_list,'all')\n",
    "valid_set_all = soccernet_dataset_generic(npy_file=valid_list,root_dir=root_dir,lang='all',lang_dict=langpath_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 190 ms, sys: 16.8 s, total: 17 s\n",
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_set_all.load_waves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 155 ms, sys: 5.78 s, total: 5.93 s\n",
      "Wall time: 5.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_set_all.generate_mel_spectrograms(load_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 83.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.1 ms, sys: 1.14 s, total: 1.2 s\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_set_all.load_resnet_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 56.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 102 ms, sys: 1.66 s, total: 1.77 s\n",
      "Wall time: 1.76 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_set_all.load_combined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.27"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((8*60)+47) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,\n",
    "         'shuffle': False,\n",
    "         'num_workers':0}\n",
    "\n",
    "# window size\n",
    "\n",
    "w = 8\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio + windowsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "def generate_model():\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1,64,kernel_size=(7,7),stride=(2,2),padding=(3,3),bias=False)\n",
    "    model.fc = nn.Linear(512,4,bias=True)\n",
    "    #resnet.features.conv0 = nn.Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    #resnet.classifier = nn.Linear(in_features=densenet.classifier.in_features, out_features=3,bias=True)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    return model, criterion,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "validloader = DataLoader(valid_set_all,**params)\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [2,4,8,16,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_load_w = []\n",
    "t_forward_w = []\n",
    "\n",
    "\n",
    "model,_,_ = generate_model()\n",
    "model.to(device)\n",
    "\n",
    "for w in windows:\n",
    "    t_gpu_load_aud = []\n",
    "    t_gpu_forward_pass_aud = []\n",
    "\n",
    "    valid_set_all.set_window_size(w)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = torch.zeros((4,4))\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            t_load_since = time.time()\n",
    "            inputs = data['ms_spot'].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "            t_load = time.time() - t_load_since\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            t_forward_pass_since = time.time()\n",
    "            outputs = model(inputs)\n",
    "            t_forward_pass = time.time() - t_forward_pass_since\n",
    "\n",
    "            t_gpu_load_aud.append(t_load)\n",
    "            t_gpu_forward_pass_aud.append(t_forward_pass) \n",
    "    t_load_w.append(t_gpu_load_aud)\n",
    "    t_forward_w.append(t_gpu_forward_pass_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 8.136504735702124e-05 0.0020615171774839743\n",
      "4 0.00010029071416610327 0.00206755858201247\n",
      "8 0.00013676692277957232 0.0023640972528702173\n",
      "16 0.00018549760182698567 0.002379000859382825\n",
      "32 0.0003183163129366361 0.0025803498732738005\n"
     ]
    }
   ],
   "source": [
    "for w,load,fp in zip(windows,t_load_w,t_forward_w):\n",
    "    avg_load = np.mean(np.array(load))\n",
    "    avg_fp = np.mean(np.array(fp))\n",
    "    print(w,avg_load,avg_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,feature_size,window_size_seconds,T_field_seconds):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        T = window_size_seconds*2\n",
    "        T_field = T_field_seconds*2\n",
    "        fc1_size = (32)*(T-(T_field-1))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,out_channels=32,kernel_size=(1,feature_size))\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
    "        self.conv2 = nn.Conv2d(1,out_channels=32,kernel_size=(T_field,32))\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n",
    "        self.fc1 = nn.Linear(fc1_size, 240)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, 240)\n",
    "        self.fc3 = nn.Linear(240, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "                \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = x.permute(0,3,2,1)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_load_w = []\n",
    "t_forward_w = []\n",
    "\n",
    "\n",
    "\n",
    "for w in windows:\n",
    "    torch.cuda.empty_cache() \n",
    "    model = Net(1024,w,w//2)\n",
    "    model.to(device)\n",
    "    t_gpu_load_aud = []\n",
    "    t_gpu_forward_pass_aud = []\n",
    "\n",
    "    valid_set_all.set_window_size(w)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = torch.zeros((4,4))\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            t_load_since = time.time()\n",
    "            inputs = data['combined_spot'].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "            t_load = time.time() - t_load_since\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            t_forward_pass_since = time.time()\n",
    "            outputs = model(inputs)\n",
    "            t_forward_pass = time.time() - t_forward_pass_since\n",
    "\n",
    "            t_gpu_load_aud.append(t_load)\n",
    "            t_gpu_forward_pass_aud.append(t_forward_pass) \n",
    "    t_load_w.append(t_gpu_load_aud)\n",
    "    t_forward_w.append(t_gpu_forward_pass_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 7.057654551970653e-05 0.0004599919685950646\n",
      "4 7.428951752491487e-05 0.0004640794411683694\n",
      "8 8.99061789879432e-05 0.0005045954386393229\n",
      "16 0.00011411776909461389 0.0005044927352514022\n",
      "32 0.00018796676244491186 0.0006227430930504433\n"
     ]
    }
   ],
   "source": [
    "for w,load,fp in zip(windows,t_load_w,t_forward_w):\n",
    "    avg_load = np.mean(np.array(load))\n",
    "    avg_fp = np.mean(np.array(fp))\n",
    "    print(w,avg_load,avg_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_load_w = []\n",
    "t_forward_w = []\n",
    "\n",
    "\n",
    "\n",
    "for w in windows:\n",
    "    torch.cuda.empty_cache() \n",
    "    model = Net(512,w,w//2)\n",
    "    model.to(device)\n",
    "    t_gpu_load_aud = []\n",
    "    t_gpu_forward_pass_aud = []\n",
    "\n",
    "    valid_set_all.set_window_size(w)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = torch.zeros((4,4))\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            t_load_since = time.time()\n",
    "            inputs = data['resnet_spot'].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "            t_load = time.time() - t_load_since\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            t_forward_pass_since = time.time()\n",
    "            outputs = model(inputs)\n",
    "            t_forward_pass = time.time() - t_forward_pass_since\n",
    "\n",
    "            t_gpu_load_aud.append(t_load)\n",
    "            t_gpu_forward_pass_aud.append(t_forward_pass) \n",
    "    t_load_w.append(t_gpu_load_aud)\n",
    "    t_forward_w.append(t_gpu_forward_pass_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,load,fp in zip(windows,t_load_w,t_forward_w):\n",
    "    avg_load = np.mean(np.array(load))\n",
    "    avg_fp = np.mean(np.array(fp))\n",
    "    print(w,avg_load,avg_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-e75db0281ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mt_forward_pass_since\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutputs_visual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisual_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_visual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutputs_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0msoftmax_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_audio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# MAKE SURE DIMS ARE CORRECT, TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0msoftmax_visual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_visual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/project-daredevil-8eBKzQn6/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_load_w = []\n",
    "t_forward_w = []\n",
    "\n",
    "\n",
    "for w in windows:\n",
    "    torch.cuda.empty_cache() \n",
    "    audio_model ,_,_ = generate_model()\n",
    "    audio_model.to(device)\n",
    "    visual_model = Net(512,w,w//2)\n",
    "    visual_model.to(device)\n",
    "    t_gpu_load_aud = []\n",
    "    t_gpu_forward_pass_aud = []\n",
    "\n",
    "    valid_set_all.set_window_size(w)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = torch.zeros((4,4))\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            t_load_since = time.time()\n",
    "            inputs_audio = data['ms_spot'].unsqueeze(1).to(device)\n",
    "            inputs_visual = data['resnet_spot'].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "            t_load = time.time() - t_load_since\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            t_forward_pass_since = time.time()\n",
    "            outputs_visual = visual_model(inputs_visual)\n",
    "            outputs_audio = audio_model(inputs_audio)\n",
    "            softmax_audio = torch.softmax(outputs_audio,dim=1) # MAKE SURE DIMS ARE CORRECT, TEST\n",
    "            softmax_visual = torch.softmax(outputs_visual,dim=1)\n",
    "            softmax_average = (softmax_audio + softmax_visual) / 2\n",
    "            \n",
    "            t_forward_pass = time.time() - t_forward_pass_since\n",
    "\n",
    "            t_gpu_load_aud.append(t_load)\n",
    "            t_gpu_forward_pass_aud.append(t_forward_pass) \n",
    "    t_load_w.append(t_gpu_load_aud)\n",
    "    t_forward_w.append(t_gpu_forward_pass_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,load,fp in zip(windows,t_load_w,t_forward_w):\n",
    "    avg_load = np.mean(np.array(load))\n",
    "    avg_fp = np.mean(np.array(fp))\n",
    "    print(w,avg_load,avg_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for w in windows:\n",
    "    \n",
    "    train_set_all.set_window_size(w)\n",
    "    valid_set_all.set_window_size(w)\n",
    "    test_set_all.set_window_size(w)\n",
    "    \n",
    "    trainloader = DataLoader(train_set_all,**params)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    testloader = DataLoader(test_set_all,**params)\n",
    "    \n",
    "    model, criterion,optimizer = generate_model()\n",
    "    # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1)\n",
    "    resnet,eng = training_helper.train_model_ws(model=model,\n",
    "           epochs=25,\n",
    "           trainloader=trainloader,\n",
    "           validloader=validloader,\n",
    "           criterion=criterion,\n",
    "           optimizer=optimizer,\n",
    "           scheduler=scheduler,\n",
    "           device=device,\n",
    "        tensorboard_name=\"gold_audio_\"+str(w))\n",
    "    \n",
    "    best_acc_valid = training_helper.evaluate_model_audio(resnet,validloader,'none',device)\n",
    "    best_acc_test = training_helper.evaluate_model_audio(resnet,testloader,'none',device)\n",
    "    torch.save(resnet.state_dict(), \"./weights/best_model_audio_\"+str(w)+\".pth\")\n",
    "    window_accs_avalid.append(best_acc_valid)\n",
    "    window_accs_atest.append(best_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.810769259929657\n",
      "Accuracy : 0.7981103658676147\n",
      "Accuracy : 0.8425641059875488\n",
      "Accuracy : 0.8438587784767151\n",
      "Accuracy : 0.870769202709198\n",
      "Accuracy : 0.8722028732299805\n",
      "Accuracy : 0.8974359035491943\n",
      "Accuracy : 0.8786673545837402\n",
      "Accuracy : 0.9066666960716248\n",
      "Accuracy : 0.8920934796333313\n"
     ]
    }
   ],
   "source": [
    "window_accs_vvalid = list()\n",
    "window_accs_vtest = list()\n",
    "\n",
    "for w in windows:\n",
    "    \n",
    "\n",
    "    train_set_all.set_window_size(w)\n",
    "    valid_set_all.set_window_size(w)\n",
    "    test_set_all.set_window_size(w)\n",
    "    \n",
    "    trainloader = DataLoader(train_set_all,**params)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    testloader = DataLoader(test_set_all,**params)\n",
    "    \n",
    "    model = Net(512,w,w//2)\n",
    "\n",
    "    model.load_state_dict(torch.load(\"./weights/best_model_visual_\"+str(w)+\".pth\"))\n",
    "    best_acc_valid = evaluate_model(model,validloader,'resnet_spot',device)\n",
    "    best_acc_test = evaluate_model(model,testloader,'resnet_spot',device)\n",
    "    #torch.save(resnet_visual.state_dict(), \"./weights/best_model_visual_v2\"+str(w)+\".pth\")\n",
    "    window_accs_vvalid.append(best_acc_valid)\n",
    "    window_accs_vtest.append(best_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,dataloader,feature_name,device):\n",
    "    #if not feature_name == 'combined_spot' or not feature_name == 'resnet_spot':\n",
    "    #    return \"Bad feature name!\"\n",
    "    running_loss=0.0\n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = torch.zeros((4,4))\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = data[feature_name].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds = torch.argmax(outputs,dim=1)\n",
    "\n",
    "            for p,gt in zip(preds,label):\n",
    "                res[int(p),int(gt)] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        N_total = res.sum()\n",
    "        N_correct = res.diag().sum()\n",
    "\n",
    "        acc = N_correct / N_total\n",
    "        print(f\"Accuracy : {acc}\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best acc\n",
      " Accuracy : 0.7702564001083374\n",
      "new best acc\n",
      " Accuracy : 0.834871768951416\n",
      "new best acc\n",
      " Accuracy : 0.8641025424003601\n",
      "new best acc\n",
      " Accuracy : 0.8733333349227905\n",
      "new best acc\n",
      " Accuracy : 0.881538450717926\n",
      "new best acc\n",
      " Accuracy : 0.8841025829315186\n",
      "new best acc\n",
      " Accuracy : 0.8851282000541687\n",
      " Accuracy : 0.8820512890815735\n",
      "new best acc\n",
      " Accuracy : 0.8912820219993591\n",
      "new best acc\n",
      " Accuracy : 0.8933333158493042\n",
      " Accuracy : 0.8917948603630066\n",
      " Accuracy : 0.8892307877540588\n",
      " Accuracy : 0.8876923322677612\n",
      " Accuracy : 0.8882051110267639\n",
      " Accuracy : 0.8887179493904114\n",
      " Accuracy : 0.8835897445678711\n",
      " Accuracy : 0.8820512890815735\n",
      " Accuracy : 0.8851282000541687\n",
      " Accuracy : 0.8887179493904114\n",
      " Accuracy : 0.8887179493904114\n",
      " Accuracy : 0.8841025829315186\n",
      " Accuracy : 0.8861538171768188\n",
      " Accuracy : 0.8866666555404663\n",
      " Accuracy : 0.882564127445221\n",
      " Accuracy : 0.8830769062042236\n",
      "Finished Training\n",
      "Accuracy : 0.8933333158493042\n",
      "Accuracy : 0.8811536431312561\n",
      "new best acc\n",
      " Accuracy : 0.6282051205635071\n",
      "new best acc\n",
      " Accuracy : 0.8097435832023621\n",
      "new best acc\n",
      " Accuracy : 0.8528205156326294\n",
      "new best acc\n",
      " Accuracy : 0.8692307472229004\n",
      "new best acc\n",
      " Accuracy : 0.8805128335952759\n",
      "new best acc\n",
      " Accuracy : 0.8835897445678711\n",
      "new best acc\n",
      " Accuracy : 0.8861538171768188\n",
      "new best acc\n",
      " Accuracy : 0.8907692432403564\n",
      " Accuracy : 0.8728204965591431\n",
      " Accuracy : 0.8897435665130615\n",
      " Accuracy : 0.8882051110267639\n",
      " Accuracy : 0.8907692432403564\n",
      " Accuracy : 0.8882051110267639\n",
      " Accuracy : 0.8892307877540588\n",
      " Accuracy : 0.890256404876709\n",
      " Accuracy : 0.8882051110267639\n",
      " Accuracy : 0.8907692432403564\n",
      " Accuracy : 0.8907692432403564\n",
      " Accuracy : 0.8887179493904114\n",
      " Accuracy : 0.8892307877540588\n",
      " Accuracy : 0.8892307877540588\n",
      " Accuracy : 0.890256404876709\n",
      " Accuracy : 0.8897435665130615\n",
      " Accuracy : 0.8897435665130615\n",
      " Accuracy : 0.8887179493904114\n",
      "Finished Training\n",
      "Accuracy : 0.8907692432403564\n",
      "Accuracy : 0.8901044130325317\n",
      "new best acc\n",
      " Accuracy : 0.5041025876998901\n",
      "new best acc\n",
      " Accuracy : 0.552307665348053\n",
      "new best acc\n",
      " Accuracy : 0.6748718023300171\n",
      "new best acc\n",
      " Accuracy : 0.7179487347602844\n",
      "new best acc\n",
      " Accuracy : 0.7353846430778503\n",
      "new best acc\n",
      " Accuracy : 0.755384624004364\n",
      " Accuracy : 0.7528204917907715\n",
      "new best acc\n",
      " Accuracy : 0.763076901435852\n",
      " Accuracy : 0.7517948746681213\n",
      " Accuracy : 0.7548717856407166\n",
      "new best acc\n",
      " Accuracy : 0.766153872013092\n",
      " Accuracy : 0.764102578163147\n",
      " Accuracy : 0.7528204917907715\n",
      " Accuracy : 0.7605128288269043\n",
      " Accuracy : 0.7620512843132019\n",
      " Accuracy : 0.7538461685180664\n",
      " Accuracy : 0.7584615349769592\n",
      " Accuracy : 0.7507692575454712\n",
      " Accuracy : 0.7579486966133118\n",
      " Accuracy : 0.7656410336494446\n",
      " Accuracy : 0.755384624004364\n",
      " Accuracy : 0.7558974623680115\n",
      " Accuracy : 0.7579486966133118\n",
      " Accuracy : 0.7502564191818237\n",
      " Accuracy : 0.7548717856407166\n",
      "Finished Training\n",
      "Accuracy : 0.766153872013092\n",
      "Accuracy : 0.7598209977149963\n"
     ]
    }
   ],
   "source": [
    "pos_accs_vvalid = list()\n",
    "pos_accs_vtest = list()\n",
    "w = 16\n",
    "for pos in window_pos:\n",
    "    \n",
    "    train_set_all.set_window_position(pos)\n",
    "    valid_set_all.set_window_position(pos)\n",
    "    test_set_all.set_window_position(pos)\n",
    "    \n",
    "    train_set_all.set_window_size(w)\n",
    "    valid_set_all.set_window_size(w)\n",
    "    test_set_all.set_window_size(w)\n",
    "    \n",
    "    trainloader = DataLoader(train_set_all,**params)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    testloader = DataLoader(test_set_all,**params)\n",
    "    \n",
    "    model = Net(512,w,w//2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1)\n",
    "    \n",
    "    \n",
    "    resnet_visual,eng = training_helper.train_model_ws_visual(model=model,\n",
    "           epochs=25,\n",
    "           trainloader=trainloader,\n",
    "           validloader=validloader,\n",
    "           criterion=criterion,\n",
    "           optimizer=optimizer,\n",
    "           scheduler=scheduler,\n",
    "           device=device,\n",
    "        tensorboard_name=\"window_size_25e_v4_\"+str(w))\n",
    "    \n",
    "    best_acc_valid = evaluate_model(resnet_visual,validloader,'resnet_spot',device)\n",
    "    best_acc_test = evaluate_model(resnet_visual,testloader,'resnet_spot',device)\n",
    "    torch.save(resnet_visual.state_dict(), \"./weights/best_model_visual_slide_\"+str(pos)+\".pth\")\n",
    "    pos_accs_vvalid.append(best_acc_valid)\n",
    "    pos_accs_vtest.append(best_acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 : Fusion prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 A) Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,feature_size,window_size_seconds,T_field_seconds):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        T = window_size_seconds*2\n",
    "        T_field = T_field_seconds*2\n",
    "        fc1_size = (32)*(T-(T_field-1))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,out_channels=32,kernel_size=(1,feature_size))\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n",
    "        self.conv2 = nn.Conv2d(1,out_channels=32,kernel_size=(T_field,32))\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n",
    "        self.fc1 = nn.Linear(fc1_size, 240)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, 240)\n",
    "        self.fc3 = nn.Linear(240, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "                \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = x.permute(0,3,2,1)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best acc\n",
      " Accuracy : 0.717435896396637\n",
      "new best acc\n",
      " Accuracy : 0.8374359011650085\n",
      " Accuracy : 0.8307692408561707\n",
      " Accuracy : 0.829230785369873\n",
      "new best acc\n",
      " Accuracy : 0.8446153998374939\n",
      " Accuracy : 0.8333333134651184\n",
      " Accuracy : 0.8276923298835754\n",
      " Accuracy : 0.8384615182876587\n",
      " Accuracy : 0.8225641250610352\n",
      " Accuracy : 0.8358974456787109\n",
      " Accuracy : 0.8323076963424683\n",
      " Accuracy : 0.8323076963424683\n",
      " Accuracy : 0.8343589901924133\n",
      " Accuracy : 0.8297435641288757\n",
      " Accuracy : 0.8410256505012512\n",
      " Accuracy : 0.8312820792198181\n",
      " Accuracy : 0.8410256505012512\n",
      " Accuracy : 0.8374359011650085\n",
      " Accuracy : 0.834871768951416\n",
      " Accuracy : 0.8389743566513062\n",
      " Accuracy : 0.8328205347061157\n",
      " Accuracy : 0.8384615182876587\n",
      " Accuracy : 0.8287179470062256\n",
      " Accuracy : 0.8333333134651184\n",
      " Accuracy : 0.8384615182876587\n",
      "Finished Training\n",
      "Accuracy : 0.8446153998374939\n",
      "Accuracy : 0.7727499008178711\n",
      "new best acc\n",
      " Accuracy : 0.6379486918449402\n",
      "new best acc\n",
      " Accuracy : 0.8574358820915222\n",
      " Accuracy : 0.854358971118927\n",
      "new best acc\n",
      " Accuracy : 0.8702564239501953\n",
      " Accuracy : 0.865128219127655\n",
      " Accuracy : 0.8641025424003601\n",
      "new best acc\n",
      " Accuracy : 0.8764102458953857\n",
      " Accuracy : 0.8615384697914124\n",
      " Accuracy : 0.8605127930641174\n",
      " Accuracy : 0.8702564239501953\n",
      " Accuracy : 0.8682051301002502\n",
      " Accuracy : 0.8671795129776001\n",
      " Accuracy : 0.8666666746139526\n",
      " Accuracy : 0.8676922917366028\n",
      " Accuracy : 0.8656409978866577\n",
      " Accuracy : 0.865128219127655\n",
      " Accuracy : 0.8676922917366028\n",
      " Accuracy : 0.8671795129776001\n",
      " Accuracy : 0.8661538362503052\n",
      " Accuracy : 0.8687179684638977\n",
      " Accuracy : 0.865128219127655\n",
      " Accuracy : 0.8697435855865479\n",
      " Accuracy : 0.8661538362503052\n",
      " Accuracy : 0.870769202709198\n",
      " Accuracy : 0.8687179684638977\n",
      "Finished Training\n",
      "Accuracy : 0.8764102458953857\n",
      "Accuracy : 0.8115365505218506\n",
      "new best acc\n",
      " Accuracy : 0.6846153736114502\n",
      "new best acc\n",
      " Accuracy : 0.8497436046600342\n",
      "new best acc\n",
      " Accuracy : 0.8661538362503052\n",
      "new best acc\n",
      " Accuracy : 0.870769202709198\n",
      "new best acc\n",
      " Accuracy : 0.8805128335952759\n",
      "new best acc\n",
      " Accuracy : 0.8851282000541687\n",
      " Accuracy : 0.8743589520454407\n",
      " Accuracy : 0.8789743781089783\n",
      " Accuracy : 0.8764102458953857\n",
      " Accuracy : 0.8799999952316284\n",
      " Accuracy : 0.879487156867981\n",
      " Accuracy : 0.881538450717926\n",
      " Accuracy : 0.8799999952316284\n",
      " Accuracy : 0.8820512890815735\n",
      " Accuracy : 0.8851282000541687\n",
      " Accuracy : 0.881538450717926\n",
      " Accuracy : 0.8805128335952759\n",
      " Accuracy : 0.8774359226226807\n",
      " Accuracy : 0.8805128335952759\n",
      " Accuracy : 0.879487156867981\n",
      " Accuracy : 0.8799999952316284\n",
      " Accuracy : 0.8810256123542786\n",
      " Accuracy : 0.8805128335952759\n",
      " Accuracy : 0.8805128335952759\n",
      " Accuracy : 0.8784615397453308\n",
      "Finished Training\n",
      "Accuracy : 0.8851282000541687\n",
      "Accuracy : 0.8090502023696899\n",
      "new best acc\n",
      " Accuracy : 0.7964102625846863\n",
      "new best acc\n",
      " Accuracy : 0.873846173286438\n",
      "new best acc\n",
      " Accuracy : 0.8943589925765991\n",
      "new best acc\n",
      " Accuracy : 0.8994871973991394\n",
      "new best acc\n",
      " Accuracy : 0.90974360704422\n",
      " Accuracy : 0.9030769467353821\n",
      " Accuracy : 0.9046154022216797\n",
      " Accuracy : 0.9082051515579224\n",
      " Accuracy : 0.890256404876709\n",
      " Accuracy : 0.908717930316925\n",
      "new best acc\n",
      " Accuracy : 0.9123076796531677\n",
      " Accuracy : 0.9076923131942749\n",
      " Accuracy : 0.908717930316925\n",
      " Accuracy : 0.908717930316925\n",
      " Accuracy : 0.9046154022216797\n",
      " Accuracy : 0.9071794748306274\n",
      " Accuracy : 0.9092307686805725\n",
      " Accuracy : 0.9056410193443298\n",
      " Accuracy : 0.90974360704422\n",
      " Accuracy : 0.9061538577079773\n",
      " Accuracy : 0.9112820625305176\n",
      " Accuracy : 0.9071794748306274\n",
      " Accuracy : 0.9071794748306274\n",
      " Accuracy : 0.9082051515579224\n",
      " Accuracy : 0.9071794748306274\n",
      "Finished Training\n",
      "Accuracy : 0.9123076796531677\n",
      "Accuracy : 0.8214818239212036\n",
      "new best acc\n",
      " Accuracy : 0.8476923108100891\n",
      "new best acc\n",
      " Accuracy : 0.8656409978866577\n",
      "new best acc\n",
      " Accuracy : 0.8912820219993591\n",
      "new best acc\n",
      " Accuracy : 0.9046154022216797\n",
      "new best acc\n",
      " Accuracy : 0.9082051515579224\n",
      "new best acc\n",
      " Accuracy : 0.9138461351394653\n",
      "new best acc\n",
      " Accuracy : 0.9189743399620056\n",
      " Accuracy : 0.9015384912490845\n",
      " Accuracy : 0.9102563858032227\n",
      "new best acc\n",
      " Accuracy : 0.9220513105392456\n",
      "new best acc\n",
      " Accuracy : 0.9235897660255432\n",
      " Accuracy : 0.9220513105392456\n",
      "new best acc\n",
      " Accuracy : 0.9251282215118408\n",
      " Accuracy : 0.9246153831481934\n",
      " Accuracy : 0.9235897660255432\n",
      " Accuracy : 0.9246153831481934\n",
      " Accuracy : 0.9225640892982483\n",
      " Accuracy : 0.9189743399620056\n",
      " Accuracy : 0.9205127954483032\n",
      " Accuracy : 0.9210256338119507\n",
      " Accuracy : 0.9210256338119507\n",
      " Accuracy : 0.9210256338119507\n",
      " Accuracy : 0.9220513105392456\n",
      " Accuracy : 0.9225640892982483\n",
      " Accuracy : 0.9200000166893005\n",
      "Finished Training\n",
      "Accuracy : 0.9251282215118408\n",
      "Accuracy : 0.8702138066291809\n"
     ]
    }
   ],
   "source": [
    "window_accs_cvalid = list()\n",
    "window_accs_ctest = list()\n",
    "\n",
    "for w in windows:\n",
    "\n",
    "    train_set_all.set_window_size(w)\n",
    "    valid_set_all.set_window_size(w)\n",
    "    test_set_all.set_window_size(w)\n",
    "    \n",
    "    trainloader = DataLoader(train_set_all,**params)\n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    testloader = DataLoader(test_set_all,**params)\n",
    "    \n",
    "\n",
    "    net = Net(1024,w,w//2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1)\n",
    "    \n",
    "    resnet_combined,eng = training_helper.train_model_ws_combined(model=net,\n",
    "           epochs=25,\n",
    "           trainloader=trainloader,\n",
    "           validloader=validloader,\n",
    "           criterion=criterion,\n",
    "           optimizer=optimizer,\n",
    "           scheduler=scheduler,\n",
    "           device=device,\n",
    "        tensorboard_name=\"comined_wsize\"+str(w))\n",
    "\n",
    "    best_acc_cval = training_helper.evaluate_model(resnet_combined,validloader,'combined_spot',device)\n",
    "    best_acc_ctest = training_helper.evaluate_model(resnet_combined,testloader,'combined_spot',device)\n",
    "    window_accs_cvalid.append(best_acc_cval)\n",
    "    window_accs_ctest.append(best_acc_ctest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 B) Softmax average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_softmax_fusion(dataloader,visual_model,audio_model):\n",
    "    visual_model.to(device)\n",
    "    audio_model.to(device)\n",
    "    visual_model.eval()\n",
    "    audio_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res_audio = torch.zeros((4,4))\n",
    "        res_visual = torch.zeros((4,4))\n",
    "        res_average = torch.zeros((4,4))\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs_audio = data['ms_spot'].unsqueeze(1).to(device)\n",
    "            inputs_visual = data['resnet_spot'].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs_audio = audio_model(inputs_audio)\n",
    "            outputs_visual = visual_model(inputs_visual)\n",
    "            \n",
    "            softmax_audio = torch.softmax(outputs_audio,dim=1) # MAKE SURE DIMS ARE CORRECT, TEST\n",
    "            softmax_visual = torch.softmax(outputs_visual,dim=1)\n",
    "            softmax_average = (softmax_audio + softmax_visual) / 2\n",
    "            \n",
    "            preds_audio = torch.argmax(outputs_audio,dim=1)\n",
    "            preds_visual = torch.argmax(outputs_visual,dim=1)\n",
    "            preds_average = torch.argmax(softmax_average,dim=1)\n",
    "\n",
    "            for p,gt in zip(preds_audio,label):\n",
    "                res_audio[int(p),int(gt)] += 1\n",
    "            \n",
    "            for p,gt in zip(preds_visual,label):\n",
    "                res_visual[int(p),int(gt)] += 1\n",
    "\n",
    "            for p,gt in zip(preds_average,label):\n",
    "                res_average[int(p),int(gt)] += 1\n",
    "                \n",
    "                \n",
    "        # Audio\n",
    "        N_total_audio = res_audio.sum()\n",
    "        N_correct_audio = res_audio.diag().sum()\n",
    "        acc_audio = N_correct_audio / N_total_audio\n",
    "        \n",
    "        N_total_visual = res_visual.sum()\n",
    "        N_correct_visual = res_visual.diag().sum()\n",
    "        acc_visual = N_correct_visual / N_total_visual\n",
    "        \n",
    "        N_total_average = res_average.sum()\n",
    "        N_correct_average = res_average.diag().sum()\n",
    "        acc_average = N_correct_average / N_total_average\n",
    "        \n",
    "        print(f\"Audio Accuracy : {acc_audio}, Visual Accuracy : {acc_visual}, Average Accuracy : {acc_average}\")\n",
    "        \n",
    "        return acc_audio,acc_visual,acc_average\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Audio Accuracy : 0.6353846192359924, Visual Accuracy : 0.810769259929657, Average Accuracy : 0.7871794700622559\n",
      "Audio Accuracy : 0.6111387610435486, Visual Accuracy : 0.7981103658676147, Average Accuracy : 0.7692690491676331\n",
      "4\n",
      "Audio Accuracy : 0.6825640797615051, Visual Accuracy : 0.8425641059875488, Average Accuracy : 0.8230769038200378\n",
      "Audio Accuracy : 0.6727995872497559, Visual Accuracy : 0.8438587784767151, Average Accuracy : 0.8239681720733643\n",
      "8\n",
      "Audio Accuracy : 0.7215384840965271, Visual Accuracy : 0.870769202709198, Average Accuracy : 0.8620513081550598\n",
      "Audio Accuracy : 0.711089015007019, Visual Accuracy : 0.8722028732299805, Average Accuracy : 0.8597712516784668\n",
      "16\n",
      "Audio Accuracy : 0.7389743328094482, Visual Accuracy : 0.8974359035491943, Average Accuracy : 0.8938461542129517\n",
      "Audio Accuracy : 0.7255097031593323, Visual Accuracy : 0.8786673545837402, Average Accuracy : 0.8920934796333313\n",
      "32\n",
      "Audio Accuracy : 0.7548717856407166, Visual Accuracy : 0.9066666960716248, Average Accuracy : 0.9205127954483032\n",
      "Audio Accuracy : 0.7389358282089233, Visual Accuracy : 0.8920934796333313, Average Accuracy : 0.9085032343864441\n"
     ]
    }
   ],
   "source": [
    "avg_w_accs_valid = list()\n",
    "avg_w_accs_test = list()\n",
    "\n",
    "for w in windows:\n",
    "    print(w)\n",
    "    valid_set_all.set_window_size(w)\n",
    "    test_set_all.set_window_size(w)\n",
    "    \n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    testloader = DataLoader(test_set_all,**params)\n",
    "    \n",
    "    # generate models\n",
    "    visual_model = Net(512,w,w//2)\n",
    "    audio_model, _,_ = generate_model()\n",
    "    \n",
    "    # load models\n",
    "    visual_model.load_state_dict(torch.load(\"./weights/best_model_visual_\"+str(w)+\".pth\"))\n",
    "    audio_model.load_state_dict(torch.load(\"./weights/best_model_audio_\"+str(w)+\".pth\"))\n",
    "    \n",
    "    avg_w_accs_valid.append(evaluate_softmax_fusion(validloader,visual_model,audio_model))\n",
    "    avg_w_accs_test.append(evaluate_softmax_fusion(testloader,visual_model,audio_model))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 32,\n",
    "         'shuffle': True,\n",
    "         'num_workers':0}\n",
    "\n",
    "# window size\n",
    "\n",
    "w = 8\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_softmax_fusion_video(dataloader,visual_model,audio_model):\n",
    "    visual_model.to(device)\n",
    "    audio_model.to(device)\n",
    "    visual_model.eval()\n",
    "    audio_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res_audio = torch.zeros((4,4))\n",
    "        res_visual = torch.zeros((4,4))\n",
    "        res_average = torch.zeros((4,4))\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs_audio = data['clip'][:, :, ::3, :, :].to(device)\n",
    "            inputs_visual = data['resnet_spot'].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs_audio = audio_model(inputs_audio)\n",
    "            outputs_visual = visual_model(inputs_visual)\n",
    "            \n",
    "            softmax_audio = torch.softmax(outputs_audio,dim=1) # MAKE SURE DIMS ARE CORRECT, TEST\n",
    "            softmax_visual = torch.softmax(outputs_visual,dim=1)\n",
    "            softmax_average = (softmax_audio + softmax_visual) / 2\n",
    "            \n",
    "            preds_audio = torch.argmax(outputs_audio,dim=1)\n",
    "            preds_visual = torch.argmax(outputs_visual,dim=1)\n",
    "            preds_average = torch.argmax(softmax_average,dim=1)\n",
    "\n",
    "            for p,gt in zip(preds_audio,label):\n",
    "                res_audio[int(p),int(gt)] += 1\n",
    "            \n",
    "            for p,gt in zip(preds_visual,label):\n",
    "                res_visual[int(p),int(gt)] += 1\n",
    "\n",
    "            for p,gt in zip(preds_average,label):\n",
    "                res_average[int(p),int(gt)] += 1\n",
    "                \n",
    "                \n",
    "        # Audio\n",
    "        N_total_audio = res_audio.sum()\n",
    "        N_correct_audio = res_audio.diag().sum()\n",
    "        acc_audio = N_correct_audio / N_total_audio\n",
    "        \n",
    "        N_total_visual = res_visual.sum()\n",
    "        N_correct_visual = res_visual.diag().sum()\n",
    "        acc_visual = N_correct_visual / N_total_visual\n",
    "        \n",
    "        N_total_average = res_average.sum()\n",
    "        N_correct_average = res_average.diag().sum()\n",
    "        acc_average = N_correct_average / N_total_average\n",
    "        \n",
    "        print(f\"Audio Accuracy : {acc_audio}, Visual Accuracy : {acc_visual}, Average Accuracy : {acc_average}\")\n",
    "        \n",
    "        return acc_audio,acc_visual,acc_average\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_accs_valid = list()\n",
    "avg_wv_accs_test = list()\n",
    "\n",
    "for w in [8,16,32]:\n",
    "    vid_valid_set.set_window_size(w)\n",
    "    vid_test_set.set_window_size(w)\n",
    "    \n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    testloader = DataLoader(test_set_all,**params)\n",
    "    \n",
    "    # generate models\n",
    "    visual_model,_,_,_ = generate_vid_model()\n",
    "    audio_model, _,_ = generate_model()\n",
    "    \n",
    "    # load models\n",
    "    visual_model.load_state_dict(torch.load(\"./weights/best_model_vid_\"+str(w)+\".pth\"))\n",
    "    audio_model.load_state_dict(torch.load(\"./weights/best_model_audio_\"+str(w)+\".pth\"))\n",
    "    \n",
    "    avg_w_accs_valid.append(evaluate_softmax_fusion_video(validloader,visual_model,audio_model))\n",
    "    avg_w_accs_test.append(evaluate_softmax_fusion_video(testloader,visual_model,audio_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 16 01:50:53 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:36:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    49W / 350W |     13MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM3...  On   | 00000000:BE:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    49W / 350W |     13MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM3...  On   | 00000000:E2:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    48W / 350W |     13MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM3...  On   | 00000000:E5:00.0 Off |                    0 |\n",
      "| N/A   51C    P0    53W / 350W |     13MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video - audio fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_softmax_fusion_video(dataloader,visual_model,audio_model):\n",
    "    visual_model.to(device)\n",
    "    audio_model.to(device)\n",
    "    visual_model.eval()\n",
    "    audio_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res_audio = torch.zeros((4,4))\n",
    "        res_visual = torch.zeros((4,4))\n",
    "        res_average = torch.zeros((4,4))\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs_audio = data['clip'][:, :, ::3, :, :].to(device)\n",
    "            inputs_visual = data['resnet_spot'].unsqueeze(1).to(device)\n",
    "            label = data['label'].to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs_audio = audio_model(inputs_audio)\n",
    "            outputs_visual = visual_model(inputs_visual)\n",
    "            \n",
    "            softmax_audio = torch.softmax(outputs_audio,dim=1) # MAKE SURE DIMS ARE CORRECT, TEST\n",
    "            softmax_visual = torch.softmax(outputs_visual,dim=1)\n",
    "            softmax_average = (softmax_audio + softmax_visual) / 2\n",
    "            \n",
    "            preds_audio = torch.argmax(outputs_audio,dim=1)\n",
    "            preds_visual = torch.argmax(outputs_visual,dim=1)\n",
    "            preds_average = torch.argmax(softmax_average,dim=1)\n",
    "\n",
    "            for p,gt in zip(preds_audio,label):\n",
    "                res_audio[int(p),int(gt)] += 1\n",
    "            \n",
    "            for p,gt in zip(preds_visual,label):\n",
    "                res_visual[int(p),int(gt)] += 1\n",
    "\n",
    "            for p,gt in zip(preds_average,label):\n",
    "                res_average[int(p),int(gt)] += 1\n",
    "                \n",
    "                \n",
    "        # Audio\n",
    "        N_total_audio = res_audio.sum()\n",
    "        N_correct_audio = res_audio.diag().sum()\n",
    "        acc_audio = N_correct_audio / N_total_audio\n",
    "        \n",
    "        N_total_visual = res_visual.sum()\n",
    "        N_correct_visual = res_visual.diag().sum()\n",
    "        acc_visual = N_correct_visual / N_total_visual\n",
    "        \n",
    "        N_total_average = res_average.sum()\n",
    "        N_correct_average = res_average.diag().sum()\n",
    "        acc_average = N_correct_average / N_total_average\n",
    "        \n",
    "        print(f\"Audio Accuracy : {acc_audio}, Visual Accuracy : {acc_visual}, Average Accuracy : {acc_average}\")\n",
    "        \n",
    "        return acc_audio,acc_visual,acc_average\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w_accs_vidvalid = list()\n",
    "avg_w_accs_vidtest = list()\n",
    "\n",
    "for w in windows:\n",
    "    print(w)\n",
    "    valid_set_all.set_window_size(w)\n",
    "    test_set_all.set_window_size(w)\n",
    "    \n",
    "    validloader = DataLoader(valid_set_all,**params)\n",
    "    testloader = DataLoader(test_set_all,**params)\n",
    "    \n",
    "    # generate models\n",
    "    visual_model = Net(512,w,w//2)\n",
    "    audio_model, _,_ = generate_model()\n",
    "    \n",
    "    # load models\n",
    "    visual_model.load_state_dict(torch.load(\"./weights/best_model_visual_\"+str(w)+\".pth\"))\n",
    "    audio_model.load_state_dict(torch.load(\"./weights/best_model_audio_\"+str(w)+\".pth\"))\n",
    "    \n",
    "    avg_w_accs_vidvalid.append(evaluate_softmax_fusion(validloader,visual_model,audio_model))\n",
    "    avg_w_accs_vidtest.append(evaluate_softmax_fusion(testloader,visual_model,audio_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ws_visual(model,epochs,trainloader,validloader,criterion,optimizer,scheduler,device,tensorboard_name):\n",
    "    running_loss=0.0\n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "\n",
    "    writer = SummaryWriter('runs/'+tensorboard_name)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        scheduler.step()\n",
    "        for i, data in enumerate(trainloader,0):\n",
    "\n",
    "            model.train()\n",
    "            inputs = data['resnet_spot'].unsqueeze(1).to(device)\n",
    "            target = data['label'].to(device)\n",
    "            inputs[inputs.isnan()] = 0.0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                running_loss += loss.item()\n",
    "                writer.add_scalar('training loss',\n",
    "                running_loss / 2000,\n",
    "                epoch * len(trainloader) + i)\n",
    "                # print statistics\n",
    "\n",
    "\n",
    "        # calculate accuracy\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            res = torch.zeros((4,4))\n",
    "            for i, data in enumerate(validloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs = data['resnet_spot'].unsqueeze(1).to(device)\n",
    "                label = data['label'].to(device)\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                preds = torch.argmax(outputs,dim=1)\n",
    "\n",
    "                for p,gt in zip(preds,label):\n",
    "                    res[int(p),int(gt)] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            N_total = res.sum()\n",
    "            N_correct = res.diag().sum()\n",
    "\n",
    "            acc = N_correct / N_total\n",
    "            if acc > best_acc:\n",
    "                print(\"new best acc\")\n",
    "                best_acc = acc\n",
    "                best_model = copy.deepcopy(model) \n",
    "\n",
    "            writer.add_scalar('accuracy validation',\n",
    "                acc,\n",
    "                epoch * len(validloader) + i)\n",
    "            print(f\" Accuracy : {acc}\")\n",
    "        \n",
    "    print('Finished Training')\n",
    "    return best_model,best_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-daredevil",
   "language": "python",
   "name": "project-daredevil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
